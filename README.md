# PySpark - PrevisÃ£o de Risco de Acidentes

Projeto de pipeline de dados com PySpark, Machine Learning (MLlib), e visualizaÃ§Ã£o interativa com Streamlit, focado na previsÃ£o de trechos rodoviÃ¡rios com maior risco de acidentes.

## ğŸ§° Tecnologias

* **Apache Spark (PySpark)**
* **scikit-learn** (para anÃ¡lises adicionais)
* **MLlib (GBTClassifier)**
* **PostgreSQL**
* **Streamlit** (interface interativa via Docker)
* **Docker & Docker Compose** (orquestraÃ§Ã£o local)
* **ReportLab** (geraÃ§Ã£o de PDFs)

## ğŸš€ Como Executar

1. Clone o repositÃ³rio:

   ```bash
   git clone https://github.com/wessantana/PySpark.git
   cd PySpark
   ```

2. Crie o arquivo `.env` na raiz do projeto com as seguintes credenciais

   ```env
   DB_USER=dbuser
   DB_PASSWORD=password
   DB_URL=url
   DB_HOST=host
   DB_PORT=5432
   DB_NAME=dbname
   SMTP_SERVER=smtp.gmail.com
   SMTP_PORT=465
   SMTP_USER=user
   SMTP_PASS=pass
   ```

3. Levante os containers com Docker Compose:

   ```bash
   docker compose up --build
   ```

   Isso iniciarÃ¡:

   * Spark (driver e executor)
   * PostgreSQL
   * Streamlit (interface em [http://localhost:8501](http://localhost:8501))

4. Acesse a interface:

   ```bash
   # No navegador
   http://localhost:8501
   ```

## ğŸ“‚ OrganizaÃ§Ã£o do Projeto

```
â”œâ”€â”€ README.md                  # Documento de entrada (este arquivo)
â”œâ”€â”€ docs/                      # DocumentaÃ§Ã£o tÃ©cnica completa
â”‚   â”œâ”€â”€ manual_tecnico.md      # DescriÃ§Ã£o do pipeline ETL e ML
â”‚   â”œâ”€â”€ relatorio_analitico.md # RelatÃ³rio de resultados e mÃ©tricas
â”œâ”€â”€ data/                      # Dados brutos e processados
â”‚   â”œâ”€â”€ raw/                   # CSVs originais
â”‚   â””â”€â”€ processed/             # Parquets transformados
|   â””â”€â”€ reports/               # Arquivo PDF gerado da anÃ¡lise
â”œâ”€â”€ src/                       # CÃ³digo-fonte do pipeline
â”‚   â”œâ”€â”€ data_collection.py     # Coleta de dados brutos
â”‚   â”œâ”€â”€ data_processing.py     # Limpeza e padronizaÃ§Ã£o
â”‚   â”œâ”€â”€ ml_pipeline.py         # CriaÃ§Ã£o de features, treinamento, prediÃ§Ã£o e relatÃ³rio
â”‚   â””â”€â”€ dashboard.py           # Dashboard Streamlit
â”œâ”€â”€ tests/                     # Testes automatizados (pytest)
â”œâ”€â”€ notebooks/                 # Notebooks de EDA
â”œâ”€â”€ requirements.txt           # DependÃªncias container Spark
â”œâ”€â”€ requirements.txt           # DependÃªncias container Streamlit
â”œâ”€â”€ docker-compose.yml         # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile                 # OrquestraÃ§Ã£o container Spark
â””â”€â”€ .env                       # VariÃ¡veis de ambiente
```

## ğŸ“ˆ Funcionalidades

* **Pipeline ETL completo**: coleta, limpeza, feature engineering e treinamento de modelo.
* **Modelo preditivo**: GBTClassifier do MLlib, com mÃ©tricas AUC-ROC e Precision\@50.
* **GeraÃ§Ã£o de relatÃ³rios PDF**: top 50 trechos crÃ­ticos, seÃ§Ã£o AnalÃ­tico e Interpretativo.
* **Interface interativa**: filtros por municÃ­pio, ano, condiÃ§Ã£o e visualizaÃ§Ã£o de mapas e tabelas.

## ğŸ›ï¸ ConfiguraÃ§Ã£o do `docker-compose.yml`

```yaml
services:
  spark:
    build: .
    container_name: spark-app
    environment:
      - DB_URL=${DB_URL}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
    volumes:
      - ./data:/app/data
      - ./src:/app/src
      - ./jars/postgresql-42.5.4.jar:/opt/bitnami/spark/jars/postgresql-42.5.4.jar
    ports:
      - "4040:4040"
    command: >
      bash -c "
      echo 'Iniciando pipeline Spark...';
      spark-submit --driver-class-path /opt/bitnami/spark/jars/postgresql-42.5.4.jar
      --jars /opt/bitnami/spark/jars/postgresql-42.5.4.jar
      /app/src/data_collection.py &&
      spark-submit --driver-class-path /opt/bitnami/spark/jars/postgresql-42.5.4.jar
      --jars /opt/bitnami/spark/jars/postgresql-42.5.4.jar
      /app/src/data_processing.py &&
      spark-submit --driver-class-path /opt/bitnami/spark/jars/postgresql-42.5.4.jar
      --jars /opt/bitnami/spark/jars/postgresql-42.5.4.jar
      /app/src/ml_pipeline.py
      "
  dashboard:
    image: python:3.11
    container_name: streamlit-app
    environment:
    - DB_URL=${DB_URL}
    - DB_USER=${DB_USER}
    - DB_PASSWORD=${DB_PASSWORD}
    depends_on:
    - spark
    working_dir: /app
    volumes:
    - ./src:/app/src
    - ./data:/app/data
    - ./dashboard-requirements.txt:/app/requirements.txt:ro
    - ./.env:/app/.env:ro
    ports:
    - "8501:8501"
    command: >
      bash -c "
      apt-get update && apt-get install -y libpq-dev &&
      pip install --upgrade pip &&
      pip install --no-cache-dir -r /app/requirements.txt &&
      streamlit run /app/src/dashboard.py --server.port=8501 --server.headless=true
      "

```

## ğŸ”§ Testes

```bash
pytest tests/
```

## ğŸ› ï¸ Futuras Melhorias

* Adicionar validaÃ§Ã£o cruzada (CrossValidator) no pipeline ML.
* Integrar XGBoost para comparaÃ§Ã£o de performance.
* Hospedar a interface no Streamlit Cloud ou outro serviÃ§o PaaS.

---
